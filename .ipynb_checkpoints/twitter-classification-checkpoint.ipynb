{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/yamasjose/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yamasjose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yamasjose/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import  SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# from nltk.classify.scikitlearn import  #SKlearnClassifier\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "np.set_printoptions(linewidth=100)\n",
    "\n",
    "%autosave 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if on google colab\n",
    "\n",
    "# import io\n",
    "# from google.colab import files \n",
    "# uploaded = files.upload()\n",
    "\n",
    "# twitter = pd.read_csv(io.StringIO(uploaded['data/gender-classifier-DFE-791531.csv'].decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = pd.read_csv('data/gender-classifier-DFE-791531.csv', encoding='latin-1' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following fields:\n",
    "\n",
    "**unit_id:** a unique id for user\n",
    "\n",
    "**golden:** whether the user was included in the gold standard for the model; TRUE or FALSE\n",
    "\n",
    "**unit_state:** state of the observation; one of finalized (for contributor-judged) or golden (for gold standard observations)\n",
    "\n",
    "**trusted_judgments:** number of trusted judgments (int); always 3 for non-golden, and what may be a unique id for gold standard observations\n",
    "\n",
    "**last_judgment_at:** date and time of last contributor judgment; blank for gold standard observations\n",
    "\n",
    "**gender:** one of male, female, or brand (for non-human profiles)\n",
    "\n",
    "**gender:confidence:** a float representing confidence in the provided gender\n",
    "\n",
    "**profile_yn:** \"no\" here seems to mean that the profile was meant to be part of the dataset but was not available when contributors went to judge it\n",
    "\n",
    "**profile_yn:confidence:** confidence in the existence/non-existence of the profile\n",
    "\n",
    "**created:** date and time when the profile was created\n",
    "\n",
    "**description:** the user's profile description\n",
    "\n",
    "**fav_number:** number of tweets the user has favorited\n",
    "\n",
    "**gender_gold:** if the profile is golden, what is the gender?\n",
    "\n",
    "**link_color:** the link color on the profile, as a hex value\n",
    "\n",
    "**name:** the user's name\n",
    "\n",
    "**profile_yn_gold:** whether the profile y/n value is golden\n",
    "\n",
    "**profileimage:** a link to the profile image\n",
    "\n",
    "**retweet_count:** number of times the user has retweeted (or possibly, been retweeted)\n",
    "\n",
    "**sidebar_color:** color of the profile sidebar, as a hex value\n",
    "\n",
    "**text:** text of a random one of the user's tweets\n",
    "\n",
    "**tweet_coord:** if the user has location turned on, the coordinates as a string with the format \"[latitude, longitude]\"\n",
    "\n",
    "**tweet_count:** number of tweets that the user has posted\n",
    "\n",
    "**tweet_created:** when the random tweet (in the text column) was created\n",
    "\n",
    "**tweet_id:** the tweet id of the random tweet\n",
    "\n",
    "**tweet_location:** location of the tweet; seems to not be particularly normalized\n",
    "\n",
    "**user_timezone:** the timezone of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>profile_yn</th>\n",
       "      <th>profile_yn:confidence</th>\n",
       "      <th>created</th>\n",
       "      <th>description</th>\n",
       "      <th>fav_number</th>\n",
       "      <th>gender_gold</th>\n",
       "      <th>link_color</th>\n",
       "      <th>name</th>\n",
       "      <th>profile_yn_gold</th>\n",
       "      <th>profileimage</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sidebar_color</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815719226</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:24</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12/5/13 1:48</td>\n",
       "      <td>i sing my own rhythm.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>08C2C2</td>\n",
       "      <td>sheezy0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/414342229...</td>\n",
       "      <td>0</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110964</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>main; @Kan1shk3</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>815719227</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:30</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/1/12 13:51</td>\n",
       "      <td>I'm the author of novels filled with family dr...</td>\n",
       "      <td>68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0084B4</td>\n",
       "      <td>DavdBurnett</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/539604221...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>ÛÏIt felt like they were my friends and I was...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7471</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>815719228</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:33</td>\n",
       "      <td>male</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11/28/14 11:30</td>\n",
       "      <td>louis whining and squealing and all</td>\n",
       "      <td>7696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABB8C2</td>\n",
       "      <td>lwtprettylaugh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/657330418...</td>\n",
       "      <td>1</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>i absolutely adore when louis starts the songs...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5617</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>clcncl</td>\n",
       "      <td>Belgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>815719229</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:10</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6/11/09 22:39</td>\n",
       "      <td>Mobile guy.  49ers, Shazam, Google, Kleiner Pe...</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0084B4</td>\n",
       "      <td>douggarland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/259703936...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>Hi @JordanSpieth - Looking at the url - do you...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1693</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>815719230</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/27/15 1:15</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4/16/14 13:23</td>\n",
       "      <td>Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...</td>\n",
       "      <td>37318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3B94D9</td>\n",
       "      <td>WilfordGemma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/564094871...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31462</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  815719226    False   finalized                   3    10/26/15 23:24   \n",
       "1  815719227    False   finalized                   3    10/26/15 23:30   \n",
       "2  815719228    False   finalized                   3    10/26/15 23:33   \n",
       "3  815719229    False   finalized                   3    10/26/15 23:10   \n",
       "4  815719230    False   finalized                   3     10/27/15 1:15   \n",
       "\n",
       "   gender  gender:confidence profile_yn  profile_yn:confidence  \\\n",
       "0    male             1.0000        yes                    1.0   \n",
       "1    male             1.0000        yes                    1.0   \n",
       "2    male             0.6625        yes                    1.0   \n",
       "3    male             1.0000        yes                    1.0   \n",
       "4  female             1.0000        yes                    1.0   \n",
       "\n",
       "          created                                        description  \\\n",
       "0    12/5/13 1:48                              i sing my own rhythm.   \n",
       "1   10/1/12 13:51  I'm the author of novels filled with family dr...   \n",
       "2  11/28/14 11:30                louis whining and squealing and all   \n",
       "3   6/11/09 22:39  Mobile guy.  49ers, Shazam, Google, Kleiner Pe...   \n",
       "4   4/16/14 13:23  Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...   \n",
       "\n",
       "   fav_number gender_gold link_color            name profile_yn_gold  \\\n",
       "0           0         NaN     08C2C2         sheezy0             NaN   \n",
       "1          68         NaN     0084B4     DavdBurnett             NaN   \n",
       "2        7696         NaN     ABB8C2  lwtprettylaugh             NaN   \n",
       "3         202         NaN     0084B4     douggarland             NaN   \n",
       "4       37318         NaN     3B94D9    WilfordGemma             NaN   \n",
       "\n",
       "                                        profileimage  retweet_count  \\\n",
       "0  https://pbs.twimg.com/profile_images/414342229...              0   \n",
       "1  https://pbs.twimg.com/profile_images/539604221...              0   \n",
       "2  https://pbs.twimg.com/profile_images/657330418...              1   \n",
       "3  https://pbs.twimg.com/profile_images/259703936...              0   \n",
       "4  https://pbs.twimg.com/profile_images/564094871...              0   \n",
       "\n",
       "  sidebar_color                                               text  \\\n",
       "0        FFFFFF  Robbie E Responds To Critics After Win Against...   \n",
       "1        C0DEED  ÛÏIt felt like they were my friends and I was...   \n",
       "2        C0DEED  i absolutely adore when louis starts the songs...   \n",
       "3        C0DEED  Hi @JordanSpieth - Looking at the url - do you...   \n",
       "4             0  Watching Neighbours on Sky+ catching up with t...   \n",
       "\n",
       "  tweet_coord  tweet_count   tweet_created      tweet_id   tweet_location  \\\n",
       "0         NaN       110964  10/26/15 12:40  6.587300e+17  main; @Kan1shk3   \n",
       "1         NaN         7471  10/26/15 12:40  6.587300e+17              NaN   \n",
       "2         NaN         5617  10/26/15 12:40  6.587300e+17           clcncl   \n",
       "3         NaN         1693  10/26/15 12:40  6.587300e+17    Palo Alto, CA   \n",
       "4         NaN        31462  10/26/15 12:40  6.587300e+17              NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                     Chennai  \n",
       "1  Eastern Time (US & Canada)  \n",
       "2                    Belgrade  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4                         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20050 entries, 0 to 20049\n",
      "Data columns (total 26 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               20050 non-null  int64  \n",
      " 1   _golden                20050 non-null  bool   \n",
      " 2   _unit_state            20050 non-null  object \n",
      " 3   _trusted_judgments     20050 non-null  int64  \n",
      " 4   _last_judgment_at      20000 non-null  object \n",
      " 5   gender                 19953 non-null  object \n",
      " 6   gender:confidence      20024 non-null  float64\n",
      " 7   profile_yn             20050 non-null  object \n",
      " 8   profile_yn:confidence  20050 non-null  float64\n",
      " 9   created                20050 non-null  object \n",
      " 10  description            16306 non-null  object \n",
      " 11  fav_number             20050 non-null  int64  \n",
      " 12  gender_gold            50 non-null     object \n",
      " 13  link_color             20050 non-null  object \n",
      " 14  name                   20050 non-null  object \n",
      " 15  profile_yn_gold        50 non-null     object \n",
      " 16  profileimage           20050 non-null  object \n",
      " 17  retweet_count          20050 non-null  int64  \n",
      " 18  sidebar_color          20050 non-null  object \n",
      " 19  text                   20050 non-null  object \n",
      " 20  tweet_coord            159 non-null    object \n",
      " 21  tweet_count            20050 non-null  int64  \n",
      " 22  tweet_created          20050 non-null  object \n",
      " 23  tweet_id               20050 non-null  float64\n",
      " 24  tweet_location         12566 non-null  object \n",
      " 25  user_timezone          12252 non-null  object \n",
      "dtypes: bool(1), float64(3), int64(5), object(17)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data and cleaning it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = twitter.drop(['_golden', '_unit_state', '_trusted_judgments', 'profile_yn', 'gender_gold', 'profile_yn_gold', 'tweet_coord', 'tweet_location', 'user_timezone'], axis = 1) \n",
    "# clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20050 entries, 0 to 20049\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               20050 non-null  int64  \n",
      " 1   _last_judgment_at      20000 non-null  object \n",
      " 2   gender                 19953 non-null  object \n",
      " 3   gender:confidence      20024 non-null  float64\n",
      " 4   profile_yn:confidence  20050 non-null  float64\n",
      " 5   created                20050 non-null  object \n",
      " 6   description            16306 non-null  object \n",
      " 7   fav_number             20050 non-null  int64  \n",
      " 8   link_color             20050 non-null  object \n",
      " 9   name                   20050 non-null  object \n",
      " 10  profileimage           20050 non-null  object \n",
      " 11  retweet_count          20050 non-null  int64  \n",
      " 12  sidebar_color          20050 non-null  object \n",
      " 13  text                   20050 non-null  object \n",
      " 14  tweet_count            20050 non-null  int64  \n",
      " 15  tweet_created          20050 non-null  object \n",
      " 16  tweet_id               20050 non-null  float64\n",
      "dtypes: float64(3), int64(4), object(10)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing data\n",
    "twitter.fillna(value = 'unknown',  \n",
    "          inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20050 entries, 0 to 20049\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               20050 non-null  int64  \n",
      " 1   _last_judgment_at      20050 non-null  object \n",
      " 2   gender                 20050 non-null  object \n",
      " 3   gender:confidence      20050 non-null  object \n",
      " 4   profile_yn:confidence  20050 non-null  float64\n",
      " 5   created                20050 non-null  object \n",
      " 6   description            20050 non-null  object \n",
      " 7   fav_number             20050 non-null  int64  \n",
      " 8   link_color             20050 non-null  object \n",
      " 9   name                   20050 non-null  object \n",
      " 10  profileimage           20050 non-null  object \n",
      " 11  retweet_count          20050 non-null  int64  \n",
      " 12  sidebar_color          20050 non-null  object \n",
      " 13  text                   20050 non-null  object \n",
      " 14  tweet_count            20050 non-null  int64  \n",
      " 15  tweet_created          20050 non-null  object \n",
      " 16  tweet_id               20050 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(11)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female     6700\n",
       "male       6194\n",
       "brand      5942\n",
       "unknown    1214\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline \n",
    "\n",
    "Below is a to do list when converting text into vector form: \n",
    "\n",
    "**Clean text and Create a Bag of Words (BoW)**\n",
    ">1. Lowercase the text\n",
    "2. Tokenize \n",
    "3. Strip out punctuation or undesirable text\n",
    "4. Remove Stopwords \n",
    "5. Stemming or Lemmatizing\n",
    "6. Compute N-Grams\n",
    "7. Use this to create BoW\n",
    "\n",
    "**Vectorize BoW**\n",
    ">8. Term Frequencies\n",
    "9. Document Frequencies\n",
    "10. TF-IDF\n",
    "11. Normalize vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, pandas.core.series.Series)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(twitter.gender), type(twitter.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female     6700\n",
      "male       6194\n",
      "brand      5942\n",
      "unknown    1214\n",
      "Name: gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classes = twitter['gender']\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tweets per gender per data\n",
    "twitter.groupby('gender').text.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of \"words\" per gender?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Consider dropping unknown to reduce uncertainty for more accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 1 1 0 2 1 1 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(classes)\n",
    "\n",
    "# print(classes[:15])\n",
    "print(Y[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Robbie E Responds To Critics After Win Against...\n",
       "1     ÛÏIt felt like they were my friends and I was...\n",
       "2     i absolutely adore when louis starts the songs...\n",
       "3     Hi @JordanSpieth - Looking at the url - do you...\n",
       "4     Watching Neighbours on Sky+ catching up with t...\n",
       "5     Ive seen people on the train with lamps, chair...\n",
       "6     @BpackEngineer Thank you for your patience whi...\n",
       "7     Gala Bingo clubs bought for å£241m: The UK's l...\n",
       "8     @_Aphmau_ the pic defines all mcd fangirls/fan...\n",
       "9     @Evielady just how lovely is the tree this yea...\n",
       "10    how are you taking care of yourself? https://t...\n",
       "11    MTG Deals 1x Rank-Up-Magic - The Seventh One -...\n",
       "12    Just put my ass on the line for you and this i...\n",
       "13    https://t.co/z4sbWUugd8 What the Nation Will B...\n",
       "14    will i even need sound effects for the diviner...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.text[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Robbie E Responds To Critics After Win Against...\n",
       "1        ÛÏIt felt like they were my friends and I was...\n",
       "2        i absolutely adore when louis starts the songs...\n",
       "3        Hi @JordanSpieth - Looking at the url - do you...\n",
       "4        Watching Neighbours on Sky+ catching up with t...\n",
       "                               ...                        \n",
       "20045    @lookupondeath ...Fine, and I'll drink tea too...\n",
       "20046    Greg Hardy you a good player and all but don't...\n",
       "20047    You can miss people and still never want to se...\n",
       "20048    @bitemyapp i had noticed your tendency to pee ...\n",
       "20049    I think for my APUSH creative project I'm goin...\n",
       "Name: text, Length: 20050, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = twitter.text\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Cleaning  text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = string.punctuation\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamasjose/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/yamasjose/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# removing @s and links\n",
    "for i,doc in enumerate(tweets):\n",
    "    tweets[i] = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", doc)\n",
    "    tweets[i] = \" \".join(tweets[i].split())\n",
    "# tweets[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamasjose/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# We will analyze not using emojis and re iterate through the same data with emojis\n",
    "\n",
    "# let's start by removing emojis and other special characters\n",
    "for i,doc in enumerate(tweets):\n",
    "    tweets[i] = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf8')\n",
    "\n",
    "# tweets, len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean punctuation\n",
    "tweets = tweets.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "# clean_tweets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean numbers\n",
    "tweets = tweets.str.replace(r'\\d+(\\.\\d+)?', ' ')\n",
    "# tweets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean whitespaces\n",
    "tweets = tweets.str.replace(r'\\s+', ' ')\n",
    "# tweets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean extra whitespaces\n",
    "tweets = tweets.str.replace(r'^\\s+|\\s+?$', '')\n",
    "# tweets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     robbie e responds to critics after win against...\n",
       "1     uiit felt like they were my friends and i was ...\n",
       "2     i absolutely adore when louis starts the songs...\n",
       "3     hi looking at the url do you use don t typical...\n",
       "4     watching neighbours on sky catching up with th...\n",
       "5     ive seen people on the train with lamps chairs...\n",
       "6     thank you for your patience while we take care...\n",
       "7     gala bingo clubs bought for a m the uk s large...\n",
       "8     the pic defines all mcd fangirls fanboys and m...\n",
       "9     just how lovely is the tree this year never se...\n",
       "10     how are you taking care of yourself fitfluential\n",
       "11    mtg deals x rank up magic the seventh one prio...\n",
       "12    just put my ass on the line for you and this i...\n",
       "13    what the nation will be talking about after we...\n",
       "14    will i even need sound effects for the diviner...\n",
       "15              it s a glow of satisfaction re the glow\n",
       "16    lmao _ua_ua dude i m hella scared for next epi...\n",
       "17    ditto i m still learning the favourites and re...\n",
       "18    i do but i don t understand how to get to the ...\n",
       "19    me too saw five lionesses drinking around the ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower\n",
    "tweets = tweets.str.lower()\n",
    "tweets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reconsider what stopwords are meaningful to not remove, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tweets = tweets.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "tweets[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and lemmatizing\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "porter_tweets = tweets.apply(lambda x: ' '.join(porter.stem(word) for word in x.split()))\n",
    "snowball_tweets = tweets.apply(lambda x: ' '.join(snowball.stem(word) for word in x.split()))\n",
    "wordnet_tweets = tweets.apply(lambda x: ' '.join(wordnet.lemmatize(word) for word in x.split()))\n",
    "\n",
    "# porter_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to call on snowball and wordnet\n",
    "bow_fd = []\n",
    "for x in porter_tweets:\n",
    "    words = word_tokenize(x)\n",
    "    for w in words:\n",
    "        bow_fd.append(w)\n",
    "        \n",
    "bow_fd = nltk.FreqDist(bow_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{}'.format(len(bow_fd)))\n",
    "print('{}'.format(bow_fd.most_common(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lim_1000 = list(bow_fd.keys())[:1000]\n",
    "word_lim_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lim_5000 = list(bow_fd.keys())[:5000]\n",
    "word_lim_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_(x, word_lim):\n",
    "    words = word_tokenize(x)\n",
    "    features = {}\n",
    "    for y in word_lim:\n",
    "        features[x] = (x in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_features = list(zip(porter_tweets, Y))\n",
    "\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(tweet_features)\n",
    "\n",
    "features = [(features_(x, word_lim_5000), marker) for (text, marker) in tweet_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = model_selection.train_test_split(features, test_size=0.2)\n",
    "\n",
    "len(training), len(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['K Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Naive Bayes']\n",
    "\n",
    "classifiers = [KNeighborsClassifier(), \n",
    "              DecisionTreeClassifier(), \n",
    "              RandomForestClassifier(), \n",
    "              MultinomialNB() \n",
    "              ]\n",
    "\n",
    "models = list(zip(names, classifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    acc = nltk.classify.accuracy(nltk_model, testing)\n",
    "    print('{}: {}'.format(name, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to call on snowball and wordnet\n",
    "bow_fd_snow = []\n",
    "for x in snowball_tweets:\n",
    "    words = word_tokenize(x)\n",
    "    for w in words:\n",
    "        bow_fd_snow.append(w)\n",
    "        \n",
    "bow_fd_snow = nltk.FreqDist(bow_fd)\n",
    "\n",
    "print('{}'.format(len(bow_fd_snow)))\n",
    "print('{}'.format(bow_fd_snow.most_common(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_features_snow = list(zip(snowball_tweets, Y))\n",
    "\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(tweet_features_snow)\n",
    "\n",
    "features_snow = [(features_(x, word_lim_5000), marker) for (text, marker) in tweet_features_snow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_snow, testing_snow = model_selection.train_test_split(features_snow, test_size=0.2)\n",
    "\n",
    "len(training_snow), len(testing_snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    nltk_model_snow = SklearnClassifier(model)\n",
    "    nltk_model_snow.train(training_snow)\n",
    "    acc_snow = nltk.classify.accuracy(nltk_model_snow, testing_snow)\n",
    "    print('{}: {}'.format(name, acc_snow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to call on snowball and wordnet\n",
    "bow_fd_wordnet = []\n",
    "for x in wordnet_tweets:\n",
    "    words = word_tokenize(x)\n",
    "    for w in words:\n",
    "        bow_fd_wordnet.append(w)\n",
    "        \n",
    "bow_fd_wordnet = nltk.FreqDist(bow_fd)\n",
    "\n",
    "print(len(bow_fd_wordnet))\n",
    "print(bow_fd_wordnet.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_features_wordnet = list(zip(wordnet_tweets, Y))\n",
    "\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(tweet_features_wordnet)\n",
    "\n",
    "features_wordnet = [(features_(x, word_lim_5000), marker) for (text, marker) in tweet_features_wordnet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_wordnet, testing_wordnet = model_selection.train_test_split(features_wordnet, test_size=0.2)\n",
    "\n",
    "len(training_wordnet), len(testing_wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    nltk_model_wordnet = SklearnClassifier(model)\n",
    "    nltk_model_wordnet.train(training_wordnet)\n",
    "    acc_wordnet = nltk.classify.accuracy(nltk_model_wordnet, testing_wordnet)\n",
    "    print('{}: ACC: {}'.format(name, acc_wordnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperation of Genders ATMP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20050 entries, 0 to 20049\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               20050 non-null  int64  \n",
      " 1   _last_judgment_at      20050 non-null  object \n",
      " 2   gender                 20050 non-null  object \n",
      " 3   gender:confidence      20050 non-null  object \n",
      " 4   profile_yn:confidence  20050 non-null  float64\n",
      " 5   created                20050 non-null  object \n",
      " 6   description            20050 non-null  object \n",
      " 7   fav_number             20050 non-null  int64  \n",
      " 8   link_color             20050 non-null  object \n",
      " 9   name                   20050 non-null  object \n",
      " 10  profileimage           20050 non-null  object \n",
      " 11  retweet_count          20050 non-null  int64  \n",
      " 12  sidebar_color          20050 non-null  object \n",
      " 13  text                   20050 non-null  object \n",
      " 14  tweet_count            20050 non-null  int64  \n",
      " 15  tweet_created          20050 non-null  object \n",
      " 16  tweet_id               20050 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(11)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female     6700\n",
       "male       6194\n",
       "brand      5942\n",
       "unknown    1214\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male = [twitter['gender'] ==  'Male']\n",
    "# male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = twitter['gender']\n",
    "# print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3cba8778c078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# X, y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "X = tweets\n",
    "y = twitter.gender\n",
    "\n",
    "# X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4599cf89ffa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# X_train, X_test, y_train, y_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipel = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('tfidf', TfidfTransformer()), \n",
    "                  ('clf', MultinomialNB())])\n",
    "\n",
    "pipel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipel.predict(X_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=twitter.gender.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restructure tweets without unknown gender and run model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20050 entries, 0 to 20049\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               20050 non-null  int64  \n",
      " 1   _last_judgment_at      20050 non-null  object \n",
      " 2   gender                 20050 non-null  object \n",
      " 3   gender:confidence      20050 non-null  object \n",
      " 4   profile_yn:confidence  20050 non-null  float64\n",
      " 5   created                20050 non-null  object \n",
      " 6   description            20050 non-null  object \n",
      " 7   fav_number             20050 non-null  int64  \n",
      " 8   link_color             20050 non-null  object \n",
      " 9   name                   20050 non-null  object \n",
      " 10  profileimage           20050 non-null  object \n",
      " 11  retweet_count          20050 non-null  int64  \n",
      " 12  sidebar_color          20050 non-null  object \n",
      " 13  text                   20050 non-null  object \n",
      " 14  tweet_count            20050 non-null  int64  \n",
      " 15  tweet_created          20050 non-null  object \n",
      " 16  tweet_id               20050 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(11)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>profile_yn:confidence</th>\n",
       "      <th>created</th>\n",
       "      <th>description</th>\n",
       "      <th>fav_number</th>\n",
       "      <th>link_color</th>\n",
       "      <th>name</th>\n",
       "      <th>profileimage</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sidebar_color</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815719226</td>\n",
       "      <td>10/26/15 23:24</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12/5/13 1:48</td>\n",
       "      <td>i sing my own rhythm.</td>\n",
       "      <td>0</td>\n",
       "      <td>08C2C2</td>\n",
       "      <td>sheezy0</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/414342229...</td>\n",
       "      <td>0</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
       "      <td>110964</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>815719227</td>\n",
       "      <td>10/26/15 23:30</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/1/12 13:51</td>\n",
       "      <td>I'm the author of novels filled with family dr...</td>\n",
       "      <td>68</td>\n",
       "      <td>0084B4</td>\n",
       "      <td>DavdBurnett</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/539604221...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>UIIt felt like they were my friends and I was ...</td>\n",
       "      <td>7471</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>815719228</td>\n",
       "      <td>10/26/15 23:33</td>\n",
       "      <td>male</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11/28/14 11:30</td>\n",
       "      <td>louis whining and squealing and all</td>\n",
       "      <td>7696</td>\n",
       "      <td>ABB8C2</td>\n",
       "      <td>lwtprettylaugh</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/657330418...</td>\n",
       "      <td>1</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>i absolutely adore when louis starts the songs...</td>\n",
       "      <td>5617</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>815719229</td>\n",
       "      <td>10/26/15 23:10</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6/11/09 22:39</td>\n",
       "      <td>Mobile guy.  49ers, Shazam, Google, Kleiner Pe...</td>\n",
       "      <td>202</td>\n",
       "      <td>0084B4</td>\n",
       "      <td>douggarland</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/259703936...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>Hi - Looking at the url - do you use Don't typ...</td>\n",
       "      <td>1693</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>815719230</td>\n",
       "      <td>10/27/15 1:15</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4/16/14 13:23</td>\n",
       "      <td>Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...</td>\n",
       "      <td>37318</td>\n",
       "      <td>3B94D9</td>\n",
       "      <td>WilfordGemma</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/564094871...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
       "      <td>31462</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id _last_judgment_at  gender gender:confidence  \\\n",
       "0  815719226    10/26/15 23:24    male                 1   \n",
       "1  815719227    10/26/15 23:30    male                 1   \n",
       "2  815719228    10/26/15 23:33    male            0.6625   \n",
       "3  815719229    10/26/15 23:10    male                 1   \n",
       "4  815719230     10/27/15 1:15  female                 1   \n",
       "\n",
       "   profile_yn:confidence         created  \\\n",
       "0                    1.0    12/5/13 1:48   \n",
       "1                    1.0   10/1/12 13:51   \n",
       "2                    1.0  11/28/14 11:30   \n",
       "3                    1.0   6/11/09 22:39   \n",
       "4                    1.0   4/16/14 13:23   \n",
       "\n",
       "                                         description  fav_number link_color  \\\n",
       "0                              i sing my own rhythm.           0     08C2C2   \n",
       "1  I'm the author of novels filled with family dr...          68     0084B4   \n",
       "2                louis whining and squealing and all        7696     ABB8C2   \n",
       "3  Mobile guy.  49ers, Shazam, Google, Kleiner Pe...         202     0084B4   \n",
       "4  Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...       37318     3B94D9   \n",
       "\n",
       "             name                                       profileimage  \\\n",
       "0         sheezy0  https://pbs.twimg.com/profile_images/414342229...   \n",
       "1     DavdBurnett  https://pbs.twimg.com/profile_images/539604221...   \n",
       "2  lwtprettylaugh  https://pbs.twimg.com/profile_images/657330418...   \n",
       "3     douggarland  https://pbs.twimg.com/profile_images/259703936...   \n",
       "4    WilfordGemma  https://pbs.twimg.com/profile_images/564094871...   \n",
       "\n",
       "   retweet_count sidebar_color  \\\n",
       "0              0        FFFFFF   \n",
       "1              0        C0DEED   \n",
       "2              1        C0DEED   \n",
       "3              0        C0DEED   \n",
       "4              0             0   \n",
       "\n",
       "                                                text  tweet_count  \\\n",
       "0  Robbie E Responds To Critics After Win Against...       110964   \n",
       "1  UIIt felt like they were my friends and I was ...         7471   \n",
       "2  i absolutely adore when louis starts the songs...         5617   \n",
       "3  Hi - Looking at the url - do you use Don't typ...         1693   \n",
       "4  Watching Neighbours on Sky+ catching up with t...        31462   \n",
       "\n",
       "    tweet_created      tweet_id  \n",
       "0  10/26/15 12:40  6.587300e+17  \n",
       "1  10/26/15 12:40  6.587300e+17  \n",
       "2  10/26/15 12:40  6.587300e+17  \n",
       "3  10/26/15 12:40  6.587300e+17  \n",
       "4  10/26/15 12:40  6.587300e+17  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_wo_uk = twitter[twitter['gender'] != 'unknown']\n",
    "# twitter_wo_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female     6700\n",
       "male       6194\n",
       "brand      5942\n",
       "unknown    1214\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_wo_uk = twitter.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>profile_yn:confidence</th>\n",
       "      <th>created</th>\n",
       "      <th>description</th>\n",
       "      <th>fav_number</th>\n",
       "      <th>link_color</th>\n",
       "      <th>name</th>\n",
       "      <th>profileimage</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sidebar_color</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815719226</td>\n",
       "      <td>10/26/15 23:24</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12/5/13 1:48</td>\n",
       "      <td>i sing my own rhythm.</td>\n",
       "      <td>0</td>\n",
       "      <td>08C2C2</td>\n",
       "      <td>sheezy0</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/414342229...</td>\n",
       "      <td>0</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
       "      <td>110964</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>815719227</td>\n",
       "      <td>10/26/15 23:30</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/1/12 13:51</td>\n",
       "      <td>I'm the author of novels filled with family dr...</td>\n",
       "      <td>68</td>\n",
       "      <td>0084B4</td>\n",
       "      <td>DavdBurnett</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/539604221...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>UIIt felt like they were my friends and I was ...</td>\n",
       "      <td>7471</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>815719228</td>\n",
       "      <td>10/26/15 23:33</td>\n",
       "      <td>male</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11/28/14 11:30</td>\n",
       "      <td>louis whining and squealing and all</td>\n",
       "      <td>7696</td>\n",
       "      <td>ABB8C2</td>\n",
       "      <td>lwtprettylaugh</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/657330418...</td>\n",
       "      <td>1</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>i absolutely adore when louis starts the songs...</td>\n",
       "      <td>5617</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>815719229</td>\n",
       "      <td>10/26/15 23:10</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6/11/09 22:39</td>\n",
       "      <td>Mobile guy.  49ers, Shazam, Google, Kleiner Pe...</td>\n",
       "      <td>202</td>\n",
       "      <td>0084B4</td>\n",
       "      <td>douggarland</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/259703936...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>Hi - Looking at the url - do you use Don't typ...</td>\n",
       "      <td>1693</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>815719230</td>\n",
       "      <td>10/27/15 1:15</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4/16/14 13:23</td>\n",
       "      <td>Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...</td>\n",
       "      <td>37318</td>\n",
       "      <td>3B94D9</td>\n",
       "      <td>WilfordGemma</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/564094871...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
       "      <td>31462</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id _last_judgment_at  gender gender:confidence  \\\n",
       "0  815719226    10/26/15 23:24    male                 1   \n",
       "1  815719227    10/26/15 23:30    male                 1   \n",
       "2  815719228    10/26/15 23:33    male            0.6625   \n",
       "3  815719229    10/26/15 23:10    male                 1   \n",
       "4  815719230     10/27/15 1:15  female                 1   \n",
       "\n",
       "   profile_yn:confidence         created  \\\n",
       "0                    1.0    12/5/13 1:48   \n",
       "1                    1.0   10/1/12 13:51   \n",
       "2                    1.0  11/28/14 11:30   \n",
       "3                    1.0   6/11/09 22:39   \n",
       "4                    1.0   4/16/14 13:23   \n",
       "\n",
       "                                         description  fav_number link_color  \\\n",
       "0                              i sing my own rhythm.           0     08C2C2   \n",
       "1  I'm the author of novels filled with family dr...          68     0084B4   \n",
       "2                louis whining and squealing and all        7696     ABB8C2   \n",
       "3  Mobile guy.  49ers, Shazam, Google, Kleiner Pe...         202     0084B4   \n",
       "4  Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...       37318     3B94D9   \n",
       "\n",
       "             name                                       profileimage  \\\n",
       "0         sheezy0  https://pbs.twimg.com/profile_images/414342229...   \n",
       "1     DavdBurnett  https://pbs.twimg.com/profile_images/539604221...   \n",
       "2  lwtprettylaugh  https://pbs.twimg.com/profile_images/657330418...   \n",
       "3     douggarland  https://pbs.twimg.com/profile_images/259703936...   \n",
       "4    WilfordGemma  https://pbs.twimg.com/profile_images/564094871...   \n",
       "\n",
       "   retweet_count sidebar_color  \\\n",
       "0              0        FFFFFF   \n",
       "1              0        C0DEED   \n",
       "2              1        C0DEED   \n",
       "3              0        C0DEED   \n",
       "4              0             0   \n",
       "\n",
       "                                                text  tweet_count  \\\n",
       "0  Robbie E Responds To Critics After Win Against...       110964   \n",
       "1  UIIt felt like they were my friends and I was ...         7471   \n",
       "2  i absolutely adore when louis starts the songs...         5617   \n",
       "3  Hi - Looking at the url - do you use Don't typ...         1693   \n",
       "4  Watching Neighbours on Sky+ catching up with t...        31462   \n",
       "\n",
       "    tweet_created      tweet_id  \n",
       "0  10/26/15 12:40  6.587300e+17  \n",
       "1  10/26/15 12:40  6.587300e+17  \n",
       "2  10/26/15 12:40  6.587300e+17  \n",
       "3  10/26/15 12:40  6.587300e+17  \n",
       "4  10/26/15 12:40  6.587300e+17  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_wo_uk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18836 entries, 0 to 20049\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               18836 non-null  int64  \n",
      " 1   _last_judgment_at      18836 non-null  object \n",
      " 2   gender                 18836 non-null  object \n",
      " 3   gender:confidence      18836 non-null  object \n",
      " 4   profile_yn:confidence  18836 non-null  float64\n",
      " 5   created                18836 non-null  object \n",
      " 6   description            18836 non-null  object \n",
      " 7   fav_number             18836 non-null  int64  \n",
      " 8   link_color             18836 non-null  object \n",
      " 9   name                   18836 non-null  object \n",
      " 10  profileimage           18836 non-null  object \n",
      " 11  retweet_count          18836 non-null  int64  \n",
      " 12  sidebar_color          18836 non-null  object \n",
      " 13  text                   18836 non-null  object \n",
      " 14  tweet_count            18836 non-null  int64  \n",
      " 15  tweet_created          18836 non-null  object \n",
      " 16  tweet_id               18836 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(11)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter_wo_uk = twitter_wo_uk[twitter_wo_uk['gender'] != 'unknown']\n",
    "twitter_wo_uk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  clean_data(tweets1):\n",
    "\n",
    "    # removing @s and links\n",
    "    for i,doc in enumerate(tweets1):\n",
    "        tweets[i] = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", doc)\n",
    "        tweets[i] = \" \".join(tweets[i].split())\n",
    "    # tweets[:15]\n",
    "\n",
    "    # We will analyze not using emojis and re iterate through the same data with emojis\n",
    "\n",
    "    # let's start by removing emojis and other special characters\n",
    "    for i,doc in enumerate(tweets1):\n",
    "        tweets1[i] = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf8')\n",
    "\n",
    "    # tweets, len(tweets)\n",
    "\n",
    "    # clean punctuation\n",
    "    tweets1 = tweets1.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "    # clean_tweets[:20]\n",
    "\n",
    "    # clean numbers\n",
    "    tweets1 = tweets1.str.replace(r'\\d+(\\.\\d+)?', ' ')\n",
    "    # tweets[:20]\n",
    "\n",
    "    # clean whitespaces\n",
    "    tweets1 = tweets1.str.replace(r'\\s+', ' ')\n",
    "    # tweets[:20]\n",
    "\n",
    "    # clean extra whitespaces\n",
    "    tweets1 = tweets1.str.replace(r'^\\s+|\\s+?$', '')\n",
    "    # tweets[:20]\n",
    "\n",
    "    # lower\n",
    "    tweets1 = tweets1.str.lower()\n",
    "    # tweets[:20]\n",
    "    \n",
    "    return tweets1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18836"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twitter_wo_uk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamasjose/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0        robbie e responds to critics after win against...\n",
       " 1        uiit felt like they were my friends and i was ...\n",
       " 2        i absolutely adore when louis starts the songs...\n",
       " 3        hi looking at the url do you use don t typical...\n",
       " 4        watching neighbours on sky catching up with th...\n",
       "                                ...                        \n",
       " 18728                  carrots and sticks startwithwhy url\n",
       " 18732                      an awkward tone and looked away\n",
       " 18778    she always with her man _u_u_u and i jus be th...\n",
       " 18830    need a ride home from practice _uo_uo_uoand it...\n",
       " 18833    you can miss people and still never want to se...\n",
       " Name: text, Length: 19970, dtype: object,\n",
       " 19970)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_wo_uk = clean_data(twitter_wo_uk.text)\n",
    "tweets_wo_uk, len(tweets_wo_uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18836, 19970)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twitter_wo_uk), len(tweets_wo_uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [19970, 18836]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-995b98a8e487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# X, y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# X_train, X_test, y_train, y_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2125\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \"\"\"\n\u001b[1;32m    291\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 256\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [19970, 18836]"
     ]
    }
   ],
   "source": [
    "X = tweets_wo_uk\n",
    "y = twitter_wo_uk.gender\n",
    "\n",
    "# X, y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# X_train, X_test, y_train, y_test\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipel = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('tfidf', TfidfTransformer()), \n",
    "                  ('clf', MultinomialNB())])\n",
    "\n",
    "pipel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipel.predict(X_test)\n",
    "# y_pred\n",
    "\n",
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=twitter.gender.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Case Study Overview with twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = twitter['text']\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Stop Words\n",
    "stop = set(stopwords.words('english'))\n",
    "# Set Stop Punctuations\n",
    "puncs = set(string.punctuation)\n",
    "# Merge Stops\n",
    "full_stop = stop.union(puncs)\n",
    "# full_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(input_str:str) -> str:\n",
    "    '''Removes accents from input string'''\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode()\n",
    "\n",
    "def filter_tokens(tokens:list, stops:object) -> list:\n",
    "    \"\"\"Filters tokens base on membership in stop list\"\"\"\n",
    "#     split_punc = lambda x: \n",
    "    res = []\n",
    "    check = [\".\", \"-\"]\n",
    "    for token in tokens:\n",
    "        if token not in stops and token.isalpha():\n",
    "            if check[0] in token:\n",
    "                res += token.partition(check[0])\n",
    "            elif check[1] in token:\n",
    "                res += token.partition(check[1])\n",
    "            else:\n",
    "                res.append(token)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Words from Documents\n",
    "tokens = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Filter each token for stop words\n",
    "doc_filter = [filter_tokens(token, full_stop) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "docs_porter = [[porter.stem(word) for word in words]\n",
    "               for words in doc_filter]\n",
    "docs_snowball = [[snowball.stem(word) for word in words]\n",
    "                 for words in doc_filter]\n",
    "docs_wordnet = [[wordnet.lemmatize(word) for word in words]\n",
    "                for words in doc_filter]\n",
    "\n",
    "# docs_porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the stemmed and lemmatized words from the first document\n",
    "print(\"%16s | %16s | %16s | %16s |\" % (\"WORD\", \"PORTER\", \"SNOWBALL\", \"LEMMATIZER\"))\n",
    "for i in range(min(len(docs_porter[0]), len(docs_snowball[0]), len(docs_wordnet[0]))):\n",
    "    p, s, w = docs_porter[0][i], docs_snowball[0][i], docs_wordnet[0][i]\n",
    "    if len(set((p, s, w))) != 1:\n",
    "        print(\"%16s | %16s | %16s | %16s |\" % (doc_filter[0][i], p, s, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem Words in Each Document\n",
    "clean_tokens = [list(map(snowball.stem, sent)) for sent in doc_filter]\n",
    "# clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for stray tokens (ones with weird puncs, not alphabetical strings)\n",
    "strays = []\n",
    "for i in range(len(clean_tokens)):\n",
    "#     print(\"--- sentence tokens (lemmatize): {}\".format(tokens_lemmatize[i]))\n",
    "    for word in clean_tokens[i]:\n",
    "        if not word.isalpha():\n",
    "            strays.append(word)\n",
    "set(strays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_series = pd.Series([\" \".join(x) for x in clean_tokens])\n",
    "document_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# term occurence = counting distinct words in each bag\n",
    "term_occ = [Counter(doc) for doc in clean_tokens]\n",
    "term_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq = list()\n",
    "for i in range(len(clean_tokens)):\n",
    "    term_freq.append( {k: (v / float(len(clean_tokens[i])))\n",
    "                       for k, v in term_occ[i].items()} )\n",
    "term_freq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_occ = Counter( [word for token in clean_tokens for word in set(token)] )\n",
    "\n",
    "doc_freq = {k: (v / float(len(clean_tokens)))\n",
    "            for k, v in doc_occ.items()}\n",
    "\n",
    "doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See words with a high frequency threshhold 50%\n",
    "thresh = 0.05\n",
    "for word, freq in doc_freq.items():\n",
    "    if freq >= thresh:\n",
    "        print(f\"{word}:  {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the minimum document frequency (in proportion of the length of the corpus)\n",
    "min_df = 0.05\n",
    "\n",
    "# filtering items to obtain the vocabulary\n",
    "vocabulary = [ k for k,v in doc_freq.items() if v >= min_df ]\n",
    "\n",
    "# print vocabulary\n",
    "print (\"-- vocabulary (len={}): {}\".format(len(vocabulary),vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Minimum Document Frequency Threshold\n",
    "x = np.arange(0.1, 1.1, 0.1)\n",
    "vocab_y = [len([ k for k,v in doc_freq.items() if v >= thresh ]) for thresh in x]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "\n",
    "ax.bar(x, vocab_y, width=0.1)\n",
    "\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_title(\"Minimum Document Frequency Thresholds\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "\n",
    "ax.set_xlabel(\"DF Threshold\")\n",
    "ax.set_ylabel(\"Length of Vocabulary\")\n",
    "\n",
    "for i, j in zip(x, vocab_y):\n",
    "    ax.axhline(j, color='r')\n",
    "\n",
    "x, vocab_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocabs = [[ k for k,v in doc_freq.items() if v >= thresh ] for thresh in x]\n",
    "for vocab in all_vocabs:\n",
    "    print(\"-- vocabulary (len={}): {}\".format(len(vocab),vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = twitter.groupby('gender')[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = CountVectorizer()\n",
    "\n",
    "document_tf_matrix = tf.fit_transform(document_series).todense()\n",
    "\n",
    "#print(tf.vocabulary_)\n",
    "#print(document_tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(frequency_matrix):\n",
    "    df =  float(len(document_tf_matrix)) / sum(frequency_matrix > 0)\n",
    "    return [log(i) for i in df.getA()[0]]\n",
    "#print(sorted(tf.vocabulary_))\n",
    "#print(idf(document_tf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "document_tfidf_matrix = tfidf.fit_transform(document_series)\n",
    "\n",
    "print(sorted(tfidf.vocabulary_))\n",
    "print(document_tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
