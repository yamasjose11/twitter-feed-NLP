{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/yamasjose/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yamasjose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yamasjose/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import  SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# from nltk.classify.scikitlearn import  #SKlearnClassifier\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "np.set_printoptions(linewidth=100)\n",
    "\n",
    "%autosave 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if on google colab\n",
    "\n",
    "# import io\n",
    "# from google.colab import files \n",
    "# uploaded = files.upload()\n",
    "\n",
    "# twitter = pd.read_csv(io.StringIO(uploaded['data/gender-classifier-DFE-791531.csv'].decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = pd.read_csv('data/gender-classifier-DFE-791531.csv', encoding='latin-1' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following fields:\n",
    "\n",
    "**unit_id:** a unique id for user\n",
    "\n",
    "**golden:** whether the user was included in the gold standard for the model; TRUE or FALSE\n",
    "\n",
    "**unit_state:** state of the observation; one of finalized (for contributor-judged) or golden (for gold standard observations)\n",
    "\n",
    "**trusted_judgments:** number of trusted judgments (int); always 3 for non-golden, and what may be a unique id for gold standard observations\n",
    "\n",
    "**last_judgment_at:** date and time of last contributor judgment; blank for gold standard observations\n",
    "\n",
    "**gender:** one of male, female, or brand (for non-human profiles)\n",
    "\n",
    "**gender:confidence:** a float representing confidence in the provided gender\n",
    "\n",
    "**profile_yn:** \"no\" here seems to mean that the profile was meant to be part of the dataset but was not available when contributors went to judge it\n",
    "\n",
    "**profile_yn:confidence:** confidence in the existence/non-existence of the profile\n",
    "\n",
    "**created:** date and time when the profile was created\n",
    "\n",
    "**description:** the user's profile description\n",
    "\n",
    "**fav_number:** number of tweets the user has favorited\n",
    "\n",
    "**gender_gold:** if the profile is golden, what is the gender?\n",
    "\n",
    "**link_color:** the link color on the profile, as a hex value\n",
    "\n",
    "**name:** the user's name\n",
    "\n",
    "**profile_yn_gold:** whether the profile y/n value is golden\n",
    "\n",
    "**profileimage:** a link to the profile image\n",
    "\n",
    "**retweet_count:** number of times the user has retweeted (or possibly, been retweeted)\n",
    "\n",
    "**sidebar_color:** color of the profile sidebar, as a hex value\n",
    "\n",
    "**text:** text of a random one of the user's tweets\n",
    "\n",
    "**tweet_coord:** if the user has location turned on, the coordinates as a string with the format \"[latitude, longitude]\"\n",
    "\n",
    "**tweet_count:** number of tweets that the user has posted\n",
    "\n",
    "**tweet_created:** when the random tweet (in the text column) was created\n",
    "\n",
    "**tweet_id:** the tweet id of the random tweet\n",
    "\n",
    "**tweet_location:** location of the tweet; seems to not be particularly normalized\n",
    "\n",
    "**user_timezone:** the timezone of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>profile_yn</th>\n",
       "      <th>profile_yn:confidence</th>\n",
       "      <th>created</th>\n",
       "      <th>description</th>\n",
       "      <th>fav_number</th>\n",
       "      <th>gender_gold</th>\n",
       "      <th>link_color</th>\n",
       "      <th>name</th>\n",
       "      <th>profile_yn_gold</th>\n",
       "      <th>profileimage</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sidebar_color</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815719226</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:24</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12/5/13 1:48</td>\n",
       "      <td>i sing my own rhythm.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>08C2C2</td>\n",
       "      <td>sheezy0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/414342229...</td>\n",
       "      <td>0</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110964</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>main; @Kan1shk3</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>815719227</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:30</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/1/12 13:51</td>\n",
       "      <td>I'm the author of novels filled with family dr...</td>\n",
       "      <td>68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0084B4</td>\n",
       "      <td>DavdBurnett</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/539604221...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>ÛÏIt felt like they were my friends and I was...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7471</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>815719228</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:33</td>\n",
       "      <td>male</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11/28/14 11:30</td>\n",
       "      <td>louis whining and squealing and all</td>\n",
       "      <td>7696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABB8C2</td>\n",
       "      <td>lwtprettylaugh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/657330418...</td>\n",
       "      <td>1</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>i absolutely adore when louis starts the songs...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5617</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>clcncl</td>\n",
       "      <td>Belgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>815719229</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:10</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6/11/09 22:39</td>\n",
       "      <td>Mobile guy.  49ers, Shazam, Google, Kleiner Pe...</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0084B4</td>\n",
       "      <td>douggarland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/259703936...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>Hi @JordanSpieth - Looking at the url - do you...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1693</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>815719230</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/27/15 1:15</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4/16/14 13:23</td>\n",
       "      <td>Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...</td>\n",
       "      <td>37318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3B94D9</td>\n",
       "      <td>WilfordGemma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/564094871...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31462</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  815719226    False   finalized                   3    10/26/15 23:24   \n",
       "1  815719227    False   finalized                   3    10/26/15 23:30   \n",
       "2  815719228    False   finalized                   3    10/26/15 23:33   \n",
       "3  815719229    False   finalized                   3    10/26/15 23:10   \n",
       "4  815719230    False   finalized                   3     10/27/15 1:15   \n",
       "\n",
       "   gender  gender:confidence profile_yn  profile_yn:confidence  \\\n",
       "0    male             1.0000        yes                    1.0   \n",
       "1    male             1.0000        yes                    1.0   \n",
       "2    male             0.6625        yes                    1.0   \n",
       "3    male             1.0000        yes                    1.0   \n",
       "4  female             1.0000        yes                    1.0   \n",
       "\n",
       "          created                                        description  \\\n",
       "0    12/5/13 1:48                              i sing my own rhythm.   \n",
       "1   10/1/12 13:51  I'm the author of novels filled with family dr...   \n",
       "2  11/28/14 11:30                louis whining and squealing and all   \n",
       "3   6/11/09 22:39  Mobile guy.  49ers, Shazam, Google, Kleiner Pe...   \n",
       "4   4/16/14 13:23  Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...   \n",
       "\n",
       "   fav_number gender_gold link_color            name profile_yn_gold  \\\n",
       "0           0         NaN     08C2C2         sheezy0             NaN   \n",
       "1          68         NaN     0084B4     DavdBurnett             NaN   \n",
       "2        7696         NaN     ABB8C2  lwtprettylaugh             NaN   \n",
       "3         202         NaN     0084B4     douggarland             NaN   \n",
       "4       37318         NaN     3B94D9    WilfordGemma             NaN   \n",
       "\n",
       "                                        profileimage  retweet_count  \\\n",
       "0  https://pbs.twimg.com/profile_images/414342229...              0   \n",
       "1  https://pbs.twimg.com/profile_images/539604221...              0   \n",
       "2  https://pbs.twimg.com/profile_images/657330418...              1   \n",
       "3  https://pbs.twimg.com/profile_images/259703936...              0   \n",
       "4  https://pbs.twimg.com/profile_images/564094871...              0   \n",
       "\n",
       "  sidebar_color                                               text  \\\n",
       "0        FFFFFF  Robbie E Responds To Critics After Win Against...   \n",
       "1        C0DEED  ÛÏIt felt like they were my friends and I was...   \n",
       "2        C0DEED  i absolutely adore when louis starts the songs...   \n",
       "3        C0DEED  Hi @JordanSpieth - Looking at the url - do you...   \n",
       "4             0  Watching Neighbours on Sky+ catching up with t...   \n",
       "\n",
       "  tweet_coord  tweet_count   tweet_created      tweet_id   tweet_location  \\\n",
       "0         NaN       110964  10/26/15 12:40  6.587300e+17  main; @Kan1shk3   \n",
       "1         NaN         7471  10/26/15 12:40  6.587300e+17              NaN   \n",
       "2         NaN         5617  10/26/15 12:40  6.587300e+17           clcncl   \n",
       "3         NaN         1693  10/26/15 12:40  6.587300e+17    Palo Alto, CA   \n",
       "4         NaN        31462  10/26/15 12:40  6.587300e+17              NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                     Chennai  \n",
       "1  Eastern Time (US & Canada)  \n",
       "2                    Belgrade  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4                         NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20050 entries, 0 to 20049\n",
      "Data columns (total 26 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               20050 non-null  int64  \n",
      " 1   _golden                20050 non-null  bool   \n",
      " 2   _unit_state            20050 non-null  object \n",
      " 3   _trusted_judgments     20050 non-null  int64  \n",
      " 4   _last_judgment_at      20000 non-null  object \n",
      " 5   gender                 19953 non-null  object \n",
      " 6   gender:confidence      20024 non-null  float64\n",
      " 7   profile_yn             20050 non-null  object \n",
      " 8   profile_yn:confidence  20050 non-null  float64\n",
      " 9   created                20050 non-null  object \n",
      " 10  description            16306 non-null  object \n",
      " 11  fav_number             20050 non-null  int64  \n",
      " 12  gender_gold            50 non-null     object \n",
      " 13  link_color             20050 non-null  object \n",
      " 14  name                   20050 non-null  object \n",
      " 15  profile_yn_gold        50 non-null     object \n",
      " 16  profileimage           20050 non-null  object \n",
      " 17  retweet_count          20050 non-null  int64  \n",
      " 18  sidebar_color          20050 non-null  object \n",
      " 19  text                   20050 non-null  object \n",
      " 20  tweet_coord            159 non-null    object \n",
      " 21  tweet_count            20050 non-null  int64  \n",
      " 22  tweet_created          20050 non-null  object \n",
      " 23  tweet_id               20050 non-null  float64\n",
      " 24  tweet_location         12566 non-null  object \n",
      " 25  user_timezone          12252 non-null  object \n",
      "dtypes: bool(1), float64(3), int64(5), object(17)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data and cleaning it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = twitter.drop(['_golden', '_unit_state', '_trusted_judgments', 'profile_yn', 'gender_gold', 'profile_yn_gold', 'tweet_coord', 'tweet_location', 'user_timezone'], axis = 1) \n",
    "# clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20050 entries, 0 to 20049\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               20050 non-null  int64  \n",
      " 1   _last_judgment_at      20000 non-null  object \n",
      " 2   gender                 19953 non-null  object \n",
      " 3   gender:confidence      20024 non-null  float64\n",
      " 4   profile_yn:confidence  20050 non-null  float64\n",
      " 5   created                20050 non-null  object \n",
      " 6   description            16306 non-null  object \n",
      " 7   fav_number             20050 non-null  int64  \n",
      " 8   link_color             20050 non-null  object \n",
      " 9   name                   20050 non-null  object \n",
      " 10  profileimage           20050 non-null  object \n",
      " 11  retweet_count          20050 non-null  int64  \n",
      " 12  sidebar_color          20050 non-null  object \n",
      " 13  text                   20050 non-null  object \n",
      " 14  tweet_count            20050 non-null  int64  \n",
      " 15  tweet_created          20050 non-null  object \n",
      " 16  tweet_id               20050 non-null  float64\n",
      "dtypes: float64(3), int64(4), object(10)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing data\n",
    "twitter.fillna(value = 'unknown',  \n",
    "          inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20050 entries, 0 to 20049\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               20050 non-null  int64  \n",
      " 1   _last_judgment_at      20050 non-null  object \n",
      " 2   gender                 20050 non-null  object \n",
      " 3   gender:confidence      20050 non-null  object \n",
      " 4   profile_yn:confidence  20050 non-null  float64\n",
      " 5   created                20050 non-null  object \n",
      " 6   description            20050 non-null  object \n",
      " 7   fav_number             20050 non-null  int64  \n",
      " 8   link_color             20050 non-null  object \n",
      " 9   name                   20050 non-null  object \n",
      " 10  profileimage           20050 non-null  object \n",
      " 11  retweet_count          20050 non-null  int64  \n",
      " 12  sidebar_color          20050 non-null  object \n",
      " 13  text                   20050 non-null  object \n",
      " 14  tweet_count            20050 non-null  int64  \n",
      " 15  tweet_created          20050 non-null  object \n",
      " 16  tweet_id               20050 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(11)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female     6700\n",
       "male       6194\n",
       "brand      5942\n",
       "unknown    1214\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline \n",
    "\n",
    "Below is a to do list when converting text into vector form: \n",
    "\n",
    "**Clean text and Create a Bag of Words (BoW)**\n",
    ">1. Lowercase the text\n",
    "2. Tokenize \n",
    "3. Strip out punctuation or undesirable text\n",
    "4. Remove Stopwords \n",
    "5. Stemming or Lemmatizing\n",
    "6. Compute N-Grams\n",
    "7. Use this to create BoW\n",
    "\n",
    "**Vectorize BoW**\n",
    ">8. Term Frequencies\n",
    "9. Document Frequencies\n",
    "10. TF-IDF\n",
    "11. Normalize vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, pandas.core.series.Series)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(twitter.gender), type(twitter.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female     6700\n",
      "male       6194\n",
      "brand      5942\n",
      "unknown    1214\n",
      "Name: gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classes = twitter['gender']\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tweets per gender per data\n",
    "twitter.groupby('gender').text.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of \"words\" per gender?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Consider dropping unknown to reduce uncertainty for more accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 1 1 0 2 1 1 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(classes)\n",
    "\n",
    "# print(classes[:15])\n",
    "print(Y[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Robbie E Responds To Critics After Win Against...\n",
       "1     ÛÏIt felt like they were my friends and I was...\n",
       "2     i absolutely adore when louis starts the songs...\n",
       "3     Hi @JordanSpieth - Looking at the url - do you...\n",
       "4     Watching Neighbours on Sky+ catching up with t...\n",
       "5     Ive seen people on the train with lamps, chair...\n",
       "6     @BpackEngineer Thank you for your patience whi...\n",
       "7     Gala Bingo clubs bought for å£241m: The UK's l...\n",
       "8     @_Aphmau_ the pic defines all mcd fangirls/fan...\n",
       "9     @Evielady just how lovely is the tree this yea...\n",
       "10    how are you taking care of yourself? https://t...\n",
       "11    MTG Deals 1x Rank-Up-Magic - The Seventh One -...\n",
       "12    Just put my ass on the line for you and this i...\n",
       "13    https://t.co/z4sbWUugd8 What the Nation Will B...\n",
       "14    will i even need sound effects for the diviner...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.text[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Robbie E Responds To Critics After Win Against...\n",
       "1        ÛÏIt felt like they were my friends and I was...\n",
       "2        i absolutely adore when louis starts the songs...\n",
       "3        Hi @JordanSpieth - Looking at the url - do you...\n",
       "4        Watching Neighbours on Sky+ catching up with t...\n",
       "                               ...                        \n",
       "20045    @lookupondeath ...Fine, and I'll drink tea too...\n",
       "20046    Greg Hardy you a good player and all but don't...\n",
       "20047    You can miss people and still never want to se...\n",
       "20048    @bitemyapp i had noticed your tendency to pee ...\n",
       "20049    I think for my APUSH creative project I'm goin...\n",
       "Name: text, Length: 20050, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = twitter.text\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Cleaning  text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = string.punctuation\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamasjose/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/yamasjose/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# removing @s and links\n",
    "for i,doc in enumerate(tweets):\n",
    "    tweets[i] = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", doc)\n",
    "    tweets[i] = \" \".join(tweets[i].split())\n",
    "# tweets[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamasjose/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# We will analyze not using emojis and re iterate through the same data with emojis\n",
    "\n",
    "# let's start by removing emojis and other special characters\n",
    "for i,doc in enumerate(tweets):\n",
    "    tweets[i] = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf8')\n",
    "\n",
    "# tweets, len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean punctuation\n",
    "tweets = tweets.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "# clean_tweets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean numbers\n",
    "tweets = tweets.str.replace(r'\\d+(\\.\\d+)?', ' ')\n",
    "# tweets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean whitespaces\n",
    "tweets = tweets.str.replace(r'\\s+', ' ')\n",
    "# tweets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean extra whitespaces\n",
    "tweets = tweets.str.replace(r'^\\s+|\\s+?$', '')\n",
    "# tweets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     robbie e responds to critics after win against...\n",
       "1     uiit felt like they were my friends and i was ...\n",
       "2     i absolutely adore when louis starts the songs...\n",
       "3     hi looking at the url do you use don t typical...\n",
       "4     watching neighbours on sky catching up with th...\n",
       "5     ive seen people on the train with lamps chairs...\n",
       "6     thank you for your patience while we take care...\n",
       "7     gala bingo clubs bought for a m the uk s large...\n",
       "8     the pic defines all mcd fangirls fanboys and m...\n",
       "9     just how lovely is the tree this year never se...\n",
       "10     how are you taking care of yourself fitfluential\n",
       "11    mtg deals x rank up magic the seventh one prio...\n",
       "12    just put my ass on the line for you and this i...\n",
       "13    what the nation will be talking about after we...\n",
       "14    will i even need sound effects for the diviner...\n",
       "15              it s a glow of satisfaction re the glow\n",
       "16    lmao _ua_ua dude i m hella scared for next epi...\n",
       "17    ditto i m still learning the favourites and re...\n",
       "18    i do but i don t understand how to get to the ...\n",
       "19    me too saw five lionesses drinking around the ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower\n",
    "tweets = tweets.str.lower()\n",
    "tweets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reconsider what stopwords are meaningful to not remove, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     robbie e responds critics win eddie edwards wo...\n",
       "1     uiit felt like friends living story themu reti...\n",
       "2     absolutely adore louis starts songs hits hard ...\n",
       "3        hi looking url use typically see advanced user\n",
       "4     watching neighbours sky catching neighbs xxx _...\n",
       "5            ive seen people train lamps chairs tvs etc\n",
       "6                        thank patience take care issue\n",
       "7     gala bingo clubs bought uk largest high street...\n",
       "8      pic defines mcd fangirls fanboys mcd shippers xd\n",
       "9     lovely tree year never seen gorgeous autumn co...\n",
       "10                             taking care fitfluential\n",
       "11    mtg deals x rank magic seventh one prio en sec...\n",
       "12                                   put ass line repay\n",
       "13        nation talking wednesday gop debates business\n",
       "14             even need sound effects diviners tonight\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tweets = tweets.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "tweets[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and lemmatizing\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "porter_tweets = tweets.apply(lambda x: ' '.join(porter.stem(word) for word in x.split()))\n",
    "snowball_tweets = tweets.apply(lambda x: ' '.join(snowball.stem(word) for word in x.split()))\n",
    "wordnet_tweets = tweets.apply(lambda x: ' '.join(wordnet.lemmatize(word) for word in x.split()))\n",
    "\n",
    "# porter_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to call on snowball and wordnet\n",
    "bow_fd = []\n",
    "for x in porter_tweets:\n",
    "    words = word_tokenize(x)\n",
    "    for w in words:\n",
    "        bow_fd.append(w)\n",
    "        \n",
    "bow_fd = nltk.FreqDist(bow_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{}'.format(len(bow_fd)))\n",
    "print('{}'.format(bow_fd.most_common(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lim_1000 = list(bow_fd.keys())[:1000]\n",
    "word_lim_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lim_5000 = list(bow_fd.keys())[:5000]\n",
    "word_lim_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_(x, word_lim):\n",
    "    words = word_tokenize(x)\n",
    "    features = {}\n",
    "    for y in word_lim:\n",
    "        features[x] = (x in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_features = list(zip(porter_tweets, Y))\n",
    "\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(tweet_features)\n",
    "\n",
    "features = [(features_(x, word_lim_5000), marker) for (text, marker) in tweet_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = model_selection.train_test_split(features, test_size=0.2)\n",
    "\n",
    "len(training), len(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['K Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Naive Bayes']\n",
    "\n",
    "classifiers = [KNeighborsClassifier(), \n",
    "              DecisionTreeClassifier(), \n",
    "              RandomForestClassifier(), \n",
    "              MultinomialNB() \n",
    "              ]\n",
    "\n",
    "models = list(zip(names, classifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    acc = nltk.classify.accuracy(nltk_model, testing)\n",
    "    print('{}: {}'.format(name, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to call on snowball and wordnet\n",
    "bow_fd_snow = []\n",
    "for x in snowball_tweets:\n",
    "    words = word_tokenize(x)\n",
    "    for w in words:\n",
    "        bow_fd_snow.append(w)\n",
    "        \n",
    "bow_fd_snow = nltk.FreqDist(bow_fd)\n",
    "\n",
    "print('{}'.format(len(bow_fd_snow)))\n",
    "print('{}'.format(bow_fd_snow.most_common(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_features_snow = list(zip(snowball_tweets, Y))\n",
    "\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(tweet_features_snow)\n",
    "\n",
    "features_snow = [(features_(x, word_lim_5000), marker) for (text, marker) in tweet_features_snow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_snow, testing_snow = model_selection.train_test_split(features_snow, test_size=0.2)\n",
    "\n",
    "len(training_snow), len(testing_snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    nltk_model_snow = SklearnClassifier(model)\n",
    "    nltk_model_snow.train(training_snow)\n",
    "    acc_snow = nltk.classify.accuracy(nltk_model_snow, testing_snow)\n",
    "    print('{}: {}'.format(name, acc_snow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to call on snowball and wordnet\n",
    "bow_fd_wordnet = []\n",
    "for x in wordnet_tweets:\n",
    "    words = word_tokenize(x)\n",
    "    for w in words:\n",
    "        bow_fd_wordnet.append(w)\n",
    "        \n",
    "bow_fd_wordnet = nltk.FreqDist(bow_fd)\n",
    "\n",
    "print(len(bow_fd_wordnet))\n",
    "print(bow_fd_wordnet.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_features_wordnet = list(zip(wordnet_tweets, Y))\n",
    "\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(tweet_features_wordnet)\n",
    "\n",
    "features_wordnet = [(features_(x, word_lim_5000), marker) for (text, marker) in tweet_features_wordnet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_wordnet, testing_wordnet = model_selection.train_test_split(features_wordnet, test_size=0.2)\n",
    "\n",
    "len(training_wordnet), len(testing_wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    nltk_model_wordnet = SklearnClassifier(model)\n",
    "    nltk_model_wordnet.train(training_wordnet)\n",
    "    acc_wordnet = nltk.classify.accuracy(nltk_model_wordnet, testing_wordnet)\n",
    "    print('{}: ACC: {}'.format(name, acc_wordnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and lemmatizing\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "# changes tweets\n",
    "porter_tweets = tweets.apply(lambda x: ' '.join(porter.stem(word) for word in x.split()))\n",
    "snowball_tweets = tweets.apply(lambda x: ' '.join(snowball.stem(word) for word in x.split()))\n",
    "wordnet_tweets = tweets.apply(lambda x: ' '.join(wordnet.lemmatize(word) for word in x.split()))\n",
    "\n",
    "# porter_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperation of Genders ATMP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20050 entries, 0 to 20049\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               20050 non-null  int64  \n",
      " 1   _last_judgment_at      20050 non-null  object \n",
      " 2   gender                 20050 non-null  object \n",
      " 3   gender:confidence      20050 non-null  object \n",
      " 4   profile_yn:confidence  20050 non-null  float64\n",
      " 5   created                20050 non-null  object \n",
      " 6   description            20050 non-null  object \n",
      " 7   fav_number             20050 non-null  int64  \n",
      " 8   link_color             20050 non-null  object \n",
      " 9   name                   20050 non-null  object \n",
      " 10  profileimage           20050 non-null  object \n",
      " 11  retweet_count          20050 non-null  int64  \n",
      " 12  sidebar_color          20050 non-null  object \n",
      " 13  text                   20050 non-null  object \n",
      " 14  tweet_count            20050 non-null  int64  \n",
      " 15  tweet_created          20050 non-null  object \n",
      " 16  tweet_id               20050 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(11)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female     6700\n",
       "male       6194\n",
       "brand      5942\n",
       "unknown    1214\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male = [twitter['gender'] ==  'Male']\n",
    "# male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = twitter['gender']\n",
    "# print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets\n",
    "y = twitter.gender\n",
    "\n",
    "# X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipel = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('tfidf', TfidfTransformer()), \n",
    "                  ('clf', MultinomialNB())])\n",
    "\n",
    "pipel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipel.predict(X_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=twitter.gender.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restructure tweets without unknown gender and run model again - new_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_wo_uk = twitter[twitter['gender'] != 'unknown']\n",
    "# twitter_wo_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_backup = twitter.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_wo_uk = twitter.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_wo_uk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_wo_uk = twitter.drop(twitter[twitter['gender'] != 'unknown'].index, inplace = True) \n",
    "# twitter_wo_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = twitter[twitter['gender'] != 'unknown']\n",
    "# new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df1 = new_df.reset_index()\n",
    "# new_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_df1.gender), len(new_df1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_data(new_df1.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_newdf = clean_data(new_df1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_newdf\n",
    "y = new_df1.gender\n",
    "\n",
    "# X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipel = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('tfidf', TfidfTransformer()), \n",
    "                  ('clf', MultinomialNB())])\n",
    "\n",
    "pipel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipel.predict(X_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=new_df1.gender.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again now without unknown and brand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_wo_uk = twitter[twitter['gender'] != 'unknown']\n",
    "# twitter_wo_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_backup = twitter.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_wo_uk = twitter.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_wo_uk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_wo_uk = twitter.drop(twitter[twitter['gender'] != 'unknown'].index, inplace = True) \n",
    "# twitter_wo_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_uk = twitter[twitter['gender'] != 'unknown']\n",
    "df_b_uk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_uk = df_b_uk.reset_index()\n",
    "df_b_uk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_uk = df_b_uk[df_b_uk['gender'] != 'brand']\n",
    "df_b_uk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_uk = df_b_uk.reset_index()\n",
    "df_b_uk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_b_uk.gender), len(df_b_uk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_uk.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_data(df_b_uk.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_b_uk = clean_data(df_b_uk.text)\n",
    "tweets_df_b_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_newdf\n",
    "y = new_df1.gender\n",
    "\n",
    "# tweets_newdf\n",
    "# new_df1\n",
    "# df_b_uk\n",
    "# X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "pipel = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('tfidf', TfidfTransformer()), \n",
    "                  ('clf', MultinomialNB())])\n",
    "\n",
    "pipel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipel.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=df_b_uk.gender.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New method - Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', SGDClassifier())])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipel2 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('tfidf', TfidfTransformer()), \n",
    "                  ('clf', SGDClassifier())])\n",
    "\n",
    "pipel2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipel2.predict(X_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.49975062344139654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       brand       0.60      0.58      0.59      1213\n",
      "      female       0.48      0.62      0.54      1307\n",
      "        male       0.43      0.39      0.41      1243\n",
      "     unknown       0.27      0.04      0.06       247\n",
      "\n",
      "    accuracy                           0.50      4010\n",
      "   macro avg       0.45      0.41      0.40      4010\n",
      "weighted avg       0.49      0.50      0.49      4010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipel.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipel2.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another New approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = twitter['gender']\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_newdf\n",
    "y = new_df1.gender\n",
    "\n",
    "# tweets_newdf\n",
    "# new_df1\n",
    "# df_b_uk\n",
    "# X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tweets_df_b_uk\n",
    "# y = df_b_uk.gender\n",
    "# #\n",
    "# print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(y)\n",
    "Y = Y.astype('int')\n",
    "# print(classes[:15])\n",
    "print(Y[:25])\n",
    "type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "# X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vect = CountVectorizer()\n",
    "# X = vect.fit_transform(tweets_df_b_uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipel3 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('tfidf', TfidfTransformer()), \n",
    "                  ('clf', RandomForestRegressor())])\n",
    "\n",
    "pipel3.fit(X_train, y_train)\n",
    "ytest = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipel3.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipel4  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipel4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  clean_data(tweets1):\n",
    "\n",
    "    # removing @s and links\n",
    "    for i,doc in enumerate(tweets1):\n",
    "        tweets[i] = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", doc)\n",
    "        tweets[i] = \" \".join(tweets[i].split())\n",
    "    # tweets[:15]\n",
    "\n",
    "    ### We will analyze not using emojis and re iterate through the same data with emojis\n",
    "\n",
    "    # let's start by removing emojis and other special characters\n",
    "    for i,doc in enumerate(tweets1):\n",
    "        tweets1[i] = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf8')\n",
    "\n",
    "    # tweets, len(tweets)\n",
    "\n",
    "    # clean punctuation\n",
    "    tweets1 = tweets1.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "    # clean_tweets[:20]\n",
    "\n",
    "    # clean numbers\n",
    "    tweets1 = tweets1.str.replace(r'\\d+(\\.\\d+)?', ' ')\n",
    "    # tweets[:20]\n",
    "\n",
    "    # clean whitespaces\n",
    "    tweets1 = tweets1.str.replace(r'\\s+', ' ')\n",
    "    # tweets[:20]\n",
    "\n",
    "    # clean extra whitespaces\n",
    "    tweets1 = tweets1.str.replace(r'^\\s+|\\s+?$', '')\n",
    "    # tweets[:20]\n",
    "\n",
    "    # lower\n",
    "    tweets1 = tweets1.str.lower()\n",
    "    # tweets[:20]\n",
    "    \n",
    "    return tweets1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = twitter[twitter['gender'] != 'unknown']\n",
    "# new_df.info()\n",
    "\n",
    "new_df1 = new_df.reset_index()\n",
    "# new_df1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamasjose/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18836"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df1.gender), len(new_df1.text)\n",
    "\n",
    "# new_df1.text\n",
    "\n",
    "len(clean_data(new_df1.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamasjose/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "tweets_newdf = clean_data(new_df1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_newdf\n",
    "y = new_df1.gender\n",
    "\n",
    "# tweets_newdf\n",
    "# new_df1\n",
    "# df_b_uk\n",
    "# X, y\n",
    "\n",
    "# X = tweets_df_b_uk\n",
    "# y = df_b_uk.gender\n",
    "# #\n",
    "# print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 1 1 0 2 1 1 0 0 1 0 1 1 1 2 2 1 1 1 2 2 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(y)\n",
    "Y = Y.astype('int')\n",
    "# print(classes[:15])\n",
    "print(Y[:25])\n",
    "type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "# X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()),\n",
       "                ('chi',\n",
       "                 SelectKBest(k=1200,\n",
       "                             score_func=<function chi2 at 0x7f47dbf21170>)),\n",
       "                ('clf', RandomForestRegressor())])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipel4 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('chi',  SelectKBest(chi2, k=1200)), \n",
    "                  ('clf', RandomForestRegressor(n_estimators=100))])\n",
    "\n",
    "pipel4.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, ..., 1, 0, 2])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest = np.array(y_test)\n",
    "ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipel4.predict(X_test)\n",
    "\n",
    "y_pred = y_pred.astype('int')\n",
    "# y_pred[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ytest), len(y_pred), type(ytest), type(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'chi', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'chi__k', 'chi__score_func', 'clf__bootstrap', 'clf__ccp_alpha', 'clf__criterion', 'clf__max_depth', 'clf__max_features', 'clf__max_leaf_nodes', 'clf__max_samples', 'clf__min_impurity_decrease', 'clf__min_impurity_split', 'clf__min_samples_leaf', 'clf__min_samples_split', 'clf__min_weight_fraction_leaf', 'clf__n_estimators', 'clf__n_jobs', 'clf__oob_score', 'clf__random_state', 'clf__verbose', 'clf__warm_start'])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipel4.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.4840764331210191\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy = ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.4840764331210191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.62      0.59      1203\n",
      "           1       0.44      0.77      0.56      1397\n",
      "           2       1.00      0.00      0.00      1168\n",
      "\n",
      "    accuracy                           0.48      3768\n",
      "   macro avg       0.67      0.46      0.38      3768\n",
      "weighted avg       0.65      0.48      0.40      3768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy = ', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-493e7ec54eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrx = confusion_matrix(ytest, y_pred)\n",
    "\n",
    "sn.heatmap(conf_matrx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", mse*(1/2.0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipel3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipel.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_wo_uk = twitter.drop[twitter['gender'] != 'unknown']\n",
    "# twitter_wo_uk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  clean_data(tweets1):\n",
    "\n",
    "    # removing @s and links\n",
    "    for i,doc in enumerate(tweets1):\n",
    "        tweets[i] = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", doc)\n",
    "        tweets[i] = \" \".join(tweets[i].split())\n",
    "    # tweets[:15]\n",
    "\n",
    "    ### We will analyze not using emojis and re iterate through the same data with emojis\n",
    "\n",
    "    # let's start by removing emojis and other special characters\n",
    "    for i,doc in enumerate(tweets1):\n",
    "        tweets1[i] = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf8')\n",
    "\n",
    "    # tweets, len(tweets)\n",
    "\n",
    "    # clean punctuation\n",
    "    tweets1 = tweets1.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "    # clean_tweets[:20]\n",
    "\n",
    "    # clean numbers\n",
    "    tweets1 = tweets1.str.replace(r'\\d+(\\.\\d+)?', ' ')\n",
    "    # tweets[:20]\n",
    "\n",
    "    # clean whitespaces\n",
    "    tweets1 = tweets1.str.replace(r'\\s+', ' ')\n",
    "    # tweets[:20]\n",
    "\n",
    "    # clean extra whitespaces\n",
    "    tweets1 = tweets1.str.replace(r'^\\s+|\\s+?$', '')\n",
    "    # tweets[:20]\n",
    "\n",
    "    # lower\n",
    "    tweets1 = tweets1.str.lower()\n",
    "    # tweets[:20]\n",
    "    \n",
    "    return tweets1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_wo_uk.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(twitter_wo_uk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(twitter_wo_uk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_wo_uk = clean_data(twitter_wo_uk.text)\n",
    "# tweets_wo_uk, len(tweets_wo_uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(twitter_wo_uk), len(tweets_wo_uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tweets_wo_uk\n",
    "# y = twitter_wo_uk.gender\n",
    "\n",
    "# # X, y\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# # X_train, X_test, y_train, y_test\n",
    "\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# pipel = Pipeline([('vect', CountVectorizer()), \n",
    "#                   ('tfidf', TfidfTransformer()), \n",
    "#                   ('clf', MultinomialNB())])\n",
    "\n",
    "# pipel.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = pipel.predict(X_test)\n",
    "# # y_pred\n",
    "\n",
    "# print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "# print(classification_report(y_test, y_pred, target_names=twitter.gender.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
